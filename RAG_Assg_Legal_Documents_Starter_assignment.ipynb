{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lf5lYawIw8tE",
   "metadata": {
    "id": "lf5lYawIw8tE"
   },
   "source": [
    "# **Extracting Information from Legal Documents Using RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NY1InIbkw80B",
   "metadata": {
    "id": "NY1InIbkw80B"
   },
   "source": [
    "## **Objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3403a4b5",
   "metadata": {
    "id": "3403a4b5"
   },
   "source": [
    "The main objective of this assignment is to process and analyse a collection text files containing legal agreements (e.g., NDAs) to prepare them for implementing a **Retrieval-Augmented Generation (RAG)** system. This involves:\n",
    "\n",
    "* Understand the Cleaned Data : Gain a comprehensive understanding of the structure, content, and context of the cleaned dataset.\n",
    "* Perform Exploratory Analysis : Conduct bivariate and multivariate analyses to uncover relationships and trends within the cleaned data.\n",
    "* Create Visualisations : Develop meaningful visualisations to support the analysis and make findings interpretable.\n",
    "* Derive Insights and Conclusions : Extract valuable insights from the cleaned data and provide clear, actionable conclusions.\n",
    "* Document the Process : Provide a detailed description of the data, its attributes, and the steps taken during the analysis for reproducibility and clarity.\n",
    "\n",
    "The ultimate goal is to transform the raw text data into a clean, structured, and analysable format that can be effectively used to build and train a RAG system for tasks like information retrieval, question-answering, and knowledge extraction related to legal agreements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3TTEcbb5hIM-",
   "metadata": {
    "id": "3TTEcbb5hIM-"
   },
   "source": [
    "### **Business Value**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZsfkEL2CgljF",
   "metadata": {
    "id": "ZsfkEL2CgljF"
   },
   "source": [
    "The project aims to leverage RAG to enhance legal document processing for businesses, law firms, and regulatory bodies. The key business objectives include:\n",
    "\n",
    "* Faster Legal Research: <br> Reduce the time lawyers and compliance officers spend searching for relevant case laws, precedents, statutes, or contract clauses.\n",
    "* Improved Contract Analysis: <br> Automatically extract key terms, obligations, and risks from lengthy contracts.\n",
    "* Regulatory Compliance Monitoring: <br> Help businesses stay updated with legal and regulatory changes by retrieving relevant legal updates.\n",
    "* Enhanced Decision-Making: <br> Provide accurate and context-aware legal insights to assist in risk assessment and legal strategy.\n",
    "\n",
    "\n",
    "**Use Cases**\n",
    "* Legal Chatbots\n",
    "* Contract Review Automation\n",
    "* Tracking Regulatory Changes and Compliance Monitoring\n",
    "* Case Law Analysis of past judgments\n",
    "* Due Diligence & Risk Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rDp_EWxVOhUu",
   "metadata": {
    "id": "rDp_EWxVOhUu"
   },
   "source": [
    "## **1. Data Loading, Preparation and Analysis** <font color=red> [20 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JZGTCfyUxalZ",
   "metadata": {
    "id": "JZGTCfyUxalZ"
   },
   "source": [
    "### **1.1 Data Understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ok6sSYNAiG8V",
   "metadata": {
    "id": "ok6sSYNAiG8V"
   },
   "source": [
    "The dataset contains legal documents and contracts collected from various sources. The documents are present as text files (`.txt`) in the *corpus* folder.\n",
    "\n",
    "There are four types of documents in the *courpus* folder, divided into four subfolders.\n",
    "- `contractnli`: contains various non-disclosure and confidentiality agreements\n",
    "- `cuad`: contains contracts with annotated legal clauses\n",
    "- `maud`: contains various merger/acquisition contracts and agreements\n",
    "- `privacy_qa`: a question-answering dataset containing privacy policies\n",
    "\n",
    "The dataset also contains evaluation files in JSON format in the *benchmark* folder. The files contain the questions and their answers, along with sources. For each of the above four folders, there is a `json` file: `contractnli.json`, `cuad.json`, `maud.json` `privacy_qa.json`. The file structure is as follows:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"tests\": [\n",
    "        {\n",
    "            \"query\": <question1>,\n",
    "            \"snippets\": [{\n",
    "                    \"file_path\": <source_file1>,\n",
    "                    \"span\": [ begin_position, end_position ],\n",
    "                    \"answer\": <relevant answer to the question 1>\n",
    "                },\n",
    "                {\n",
    "                    \"file_path\": <source_file2>,\n",
    "                    \"span\": [ begin_position, end_position ],\n",
    "                    \"answer\": <relevant answer to the question 2>\n",
    "                }, ....\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"query\": <question2>,\n",
    "            \"snippets\": [{<answer context for que 2>}]\n",
    "        },\n",
    "        ... <more queries>\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S7Ac8VxvjWnw",
   "metadata": {
    "id": "S7Ac8VxvjWnw"
   },
   "source": [
    "### **1.2 Load and Preprocess the data** <font color=red> [5 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gJ8fA4Nh3fHg",
   "metadata": {
    "id": "gJ8fA4Nh3fHg"
   },
   "source": [
    "#### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "BqyFHhSn48tC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BqyFHhSn48tC",
    "outputId": "924635b1-69b6-4b40-cb2a-a03250e19f4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\kaash\\anaconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\kaash\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\kaash\\anaconda3\\lib\\site-packages (from faiss-cpu) (23.2)\n"
     ]
    }
   ],
   "source": [
    "## The following libraries might be useful\n",
    "!pip install -q langchain-openai\n",
    "!pip install -U -q langchain-community\n",
    "!pip install -U -q langchain-chroma\n",
    "!pip install -U -q datasets\n",
    "!pip install -U -q ragas\n",
    "!pip install -U -q rouge_score\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "Qpn-qbhAi58F",
   "metadata": {
    "id": "Qpn-qbhAi58F"
   },
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "\n",
    "# Basic Python utilities\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# LangChain core components\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# NLTK \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# SK-Learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# API\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Visualization and display\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set your OpenAI API key (make sure to use environment variables in production)\n",
    "import openai\n",
    "# openai.api_key = \"your-api-key\"  \n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "\n",
    "# Optional: for cleaning or preprocessing text\n",
    "import re\n",
    "import string\n",
    "import zipfile\n",
    "import random\n",
    "import logging\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zOMf-tfIiOlp",
   "metadata": {
    "id": "zOMf-tfIiOlp"
   },
   "source": [
    "#### **1.2.1** <font color=red> [3 marks] </font>\n",
    "Load all `.txt` files from the folders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea36ba",
   "metadata": {
    "id": "f2ea36ba"
   },
   "source": [
    "You can utilise document loaders from the options provided by the LangChain community.\n",
    "\n",
    "Optionally, you can also read the files manually, while ensuring proper handling of encoding issues (e.g., utf-8, latin1). In such case, also store the file content along with metadata (e.g., file name, directory path) for traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6QdfsZiVOHY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6QdfsZiVOHY",
    "outputId": "67ee2441-6cdf-4627-ae99-402d131500e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.13/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-community in /opt/anaconda3/lib/python3.13/site-packages (0.3.30)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /opt/anaconda3/lib/python3.13/site-packages (from langchain) (0.3.76)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /opt/anaconda3/lib/python3.13/site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /opt/anaconda3/lib/python3.13/site-packages (from langchain) (0.4.31)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.13/site-packages (from langchain) (2.10.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.13/site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.13/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.12.2)\n",
      "Requirement already satisfied: packaging>=23.2 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-community) (3.11.10)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=2.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-community) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.13/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "I9rTY8DWx2Wj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9rTY8DWx2Wj",
    "outputId": "0b4c7bba-9ff4-4d72-abd4-f72cdfdd9783"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted to: rag_legal_data\n",
      "Total .txt files loaded: 698\n",
      "📄 Sample metadata from first document: {'source': 'rag_legal_data/rag_legal/corpus/maud/The Michaels Companies, Inc._Apollo Global Management, LLC.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Load the files as documents\n",
    "\n",
    "# Step 1: Extract the zip file\n",
    "zip_path = \"rag_legal.zip\"\n",
    "extract_dir = \"rag_legal_data\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(f\"Extracted to: {extract_dir}\")\n",
    "\n",
    "# Step 2: Load all .txt files using LangChain\n",
    "loader = DirectoryLoader(\n",
    "    path=extract_dir,\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 3: Sanity check\n",
    "print(f\"Total .txt files loaded: {len(documents)}\")\n",
    "print(\"📄 Sample metadata from first document:\", documents[0].metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K4HYLoUjwmMs",
   "metadata": {
    "id": "K4HYLoUjwmMs"
   },
   "source": [
    "#### **1.2.2** <font color=red> [2 marks] </font>\n",
    "Preprocess the text data to remove noise and prepare it for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9793fdf",
   "metadata": {
    "id": "e9793fdf"
   },
   "source": [
    "Remove special characters, extra whitespace, and irrelevant content such as email and telephone contact info.\n",
    "Normalise text (e.g., convert to lowercase, remove stop words).\n",
    "Handle missing or corrupted data by logging errors and skipping problematic files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ec87e69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ec87e69",
    "outputId": "7c7c3ec6-9013-4877-fc72-52b1ba3f5c31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shekharanand/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shekharanand/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Clean and preprocess the data\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "def clean_text(text):\n",
    "    try:\n",
    "        if pd.isnull(text):\n",
    "            return \"\"\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove emails\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "        # Remove phone numbers\n",
    "        text = re.sub(r'\\b\\d{10,}\\b', '', text)\n",
    "        text = re.sub(r'\\(?\\+?\\d{1,3}?\\)?[-.\\s]?\\d{3,5}[-.\\s]?\\d{3,5}[-.\\s]?\\d{3,5}', '', text)\n",
    "\n",
    "        # Remove special characters and punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # Remove extra whitespaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        # Tokenize and remove stopwords\n",
    "        tokens = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Example usage on a DataFrame column\n",
    "# Assuming you have a DataFrame `df` and a text column named 'text'\n",
    "# df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e90470",
   "metadata": {
    "id": "b9e90470"
   },
   "source": [
    "### **1.3 Exploratory Data Analysis** <font color=red> [10 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Nd1K4yhIzyPp",
   "metadata": {
    "id": "Nd1K4yhIzyPp"
   },
   "source": [
    "#### **1.3.1** <font color=red> [1 marks] </font>\n",
    "Calculate the average, maximum and minimum document length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "tQT1UIcOHSp9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tQT1UIcOHSp9",
    "outputId": "6a52ea50-0a6a-41c6-aab0-a09d0552970e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average, maximum and minimum document length.\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess_text(text):\n",
    "    try:\n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        # Remove phone numbers (basic patterns)\n",
    "        text = re.sub(r'\\b\\d{10}\\b|\\b\\d{3}[-.\\s]??\\d{3}[-.\\s]??\\d{4}\\b', '', text)\n",
    "        # Remove special characters (except basic punctuation)\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s.,]', '', text)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Apply the cleaning function to all documents\n",
    "for doc in documents:\n",
    "    doc.page_content = preprocess_text(doc.page_content)\n",
    "\n",
    "print(\"Preprocessing complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18xQu__O0wLv",
   "metadata": {
    "id": "18xQu__O0wLv"
   },
   "source": [
    "#### **1.3.2** <font color=red> [4 marks] </font>\n",
    "Analyse the frequency of occurrence of words and find the most and least occurring words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IQ_i5YfFH2dg",
   "metadata": {
    "id": "IQ_i5YfFH2dg"
   },
   "source": [
    "Find the 20 most common and least common words in the text. Ignore stop words such as articles and prepositions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "Q8eiDTy2Ic8z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8eiDTy2Ic8z",
    "outputId": "c76a2c26-44e9-46e5-b21a-9c057985c035"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shekharanand/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Most Common Words:\n",
      "\n",
      "company: 133429\n",
      "shall: 104675\n",
      "section: 74878\n",
      "agreement: 69536\n",
      "parent: 49649\n",
      "party: 43218\n",
      "material: 33428\n",
      "date: 31137\n",
      "merger: 29332\n",
      "respect: 28412\n",
      "may: 26997\n",
      "applicable: 26583\n",
      "including: 26410\n",
      "subsidiaries: 25674\n",
      "time: 23924\n",
      "prior: 23316\n",
      "agreement,: 22781\n",
      "stock: 22563\n",
      "information: 21643\n",
      "effective: 21070\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "20 Least Common Words:\n",
      "\n",
      "mason,: 1\n",
      "hnba.: 1\n",
      "deliberations,: 1\n",
      "colloquy,: 1\n",
      "guangdong.: 1\n",
      "liuxian: 1\n",
      "nanshanyungu: 1\n",
      "shanshui: 1\n",
      "address1f,: 1\n",
      "zhen,: 1\n",
      "shen: 1\n",
      "1,0000,000: 1\n",
      "nonbeaching: 1\n",
      "publicizing.: 1\n",
      "coded,: 1\n",
      "interested.: 1\n",
      "seeed,: 1\n",
      "chai,: 1\n",
      "lockhart: 1\n",
      "315321: 1\n"
     ]
    }
   ],
   "source": [
    "# Find frequency of occurence of words\n",
    "\n",
    "\n",
    "# Download stopwords if not already present\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define English stopwords set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Combine all text from documents\n",
    "all_text = \" \".join(doc.page_content for doc in documents)\n",
    "\n",
    "# Tokenise the text\n",
    "words = all_text.split()\n",
    "\n",
    "# Remove stop words and short words (e.g. single characters)\n",
    "filtered_words = [word for word in words if word not in stop_words and len(word) > 1]\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = Counter(filtered_words)\n",
    "\n",
    "# Get 20 most common and 20 least common (excluding ties)\n",
    "most_common = word_freq.most_common(20)\n",
    "least_common = word_freq.most_common()[:-21:-1]\n",
    "\n",
    "# Display results\n",
    "print(\"\\nTop 20 Most Common Words:\\n\")\n",
    "for word, freq in most_common:\n",
    "    print(f\"{word}: {freq}\")\n",
    "print(\"\\n-------------------------------\")\n",
    "print(\"\\n20 Least Common Words:\\n\")\n",
    "for word, freq in least_common:\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xlF55RNjz9pQ",
   "metadata": {
    "id": "xlF55RNjz9pQ"
   },
   "source": [
    "#### **1.3.3** <font color=red> [4 marks] </font>\n",
    "Analyse the similarity of different documents to each other based on TF-IDF vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jciCNMelOGPJ",
   "metadata": {
    "id": "jciCNMelOGPJ"
   },
   "source": [
    "Transform some documents to TF-IDF vectors and calculate their similarity matrix using a suitable distance function. If contracts contain duplicate or highly similar clauses, similarity calculation can help detect them.\n",
    "\n",
    "Identify for the first 10 documents and then for 10 random documents. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "M-_SrvDcMnKi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M-_SrvDcMnKi",
    "outputId": "a9802f4e-510d-40ce-fbaa-caf5b0a329bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Matrix for First 10 Documents:\n",
      "\n",
      "[[1.   0.93 0.99 0.99 0.95 0.99 0.99 0.92 0.99 0.99]\n",
      " [0.93 1.   0.93 0.93 0.9  0.93 0.93 0.87 0.92 0.92]\n",
      " [0.99 0.93 1.   0.99 0.96 0.99 0.99 0.92 0.99 0.99]\n",
      " [0.99 0.93 0.99 1.   0.96 0.99 0.99 0.92 0.99 0.99]\n",
      " [0.95 0.9  0.96 0.96 1.   0.96 0.95 0.89 0.96 0.95]\n",
      " [0.99 0.93 0.99 0.99 0.96 1.   0.99 0.92 0.99 1.  ]\n",
      " [0.99 0.93 0.99 0.99 0.95 0.99 1.   0.92 0.99 0.99]\n",
      " [0.92 0.87 0.92 0.92 0.89 0.92 0.92 1.   0.92 0.92]\n",
      " [0.99 0.92 0.99 0.99 0.96 0.99 0.99 0.92 1.   0.99]\n",
      " [0.99 0.92 0.99 0.99 0.95 1.   0.99 0.92 0.99 1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Transform the page contents of documents\n",
    "\n",
    "# Extract content from first 10 documents\n",
    "first_10_docs = [doc.page_content for doc in documents[:10]]\n",
    "\n",
    "# TF-IDF vectorisation\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(first_10_docs)\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarity_matrix_first_10 = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Display similarity matrix\n",
    "print(\"Similarity Matrix for First 10 Documents:\\n\")\n",
    "print(np.round(similarity_matrix_first_10, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "pd99eXtnK2DU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pd99eXtnK2DU",
    "outputId": "edc9f0d7-856b-43a3-d4cc-b204060da129"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Document Indices: [404, 400, 602, 371, 557, 166, 1, 298, 501, 232]\n"
     ]
    }
   ],
   "source": [
    "# create a list of 10 random integers\n",
    "\n",
    "# Create a list of 10 unique random indices from available documents\n",
    "random_indices = random.sample(range(len(documents)), 10)\n",
    "print(\"Random Document Indices:\", random_indices)\n",
    "\n",
    "# Extract corresponding page contents\n",
    "random_docs = [documents[i].page_content for i in random_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "t31ngfZTJimS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t31ngfZTJimS",
    "outputId": "000f5617-9d1e-4bd0-b856-590ede5893a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Matrix for 10 Random Documents:\n",
      "\n",
      "[[1.   0.53 0.68 0.73 0.63 0.59 0.78 0.65 0.52 0.7 ]\n",
      " [0.53 1.   0.54 0.54 0.48 0.45 0.55 0.49 0.42 0.52]\n",
      " [0.68 0.54 1.   0.67 0.59 0.55 0.71 0.61 0.5  0.65]\n",
      " [0.73 0.54 0.67 1.   0.63 0.59 0.77 0.65 0.55 0.72]\n",
      " [0.63 0.48 0.59 0.63 1.   0.52 0.66 0.57 0.46 0.7 ]\n",
      " [0.59 0.45 0.55 0.59 0.52 1.   0.61 0.53 0.44 0.58]\n",
      " [0.78 0.55 0.71 0.77 0.66 0.61 1.   0.67 0.54 0.73]\n",
      " [0.65 0.49 0.61 0.65 0.57 0.53 0.67 1.   0.54 0.64]\n",
      " [0.52 0.42 0.5  0.55 0.46 0.44 0.54 0.54 1.   0.53]\n",
      " [0.7  0.52 0.65 0.72 0.7  0.58 0.73 0.64 0.53 1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity scores for 10 random documents\n",
    "# Vectorise the random documents\n",
    "tfidf_matrix_random = vectorizer.fit_transform(random_docs)\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarity_matrix_random = cosine_similarity(tfidf_matrix_random)\n",
    "\n",
    "# Display similarity matrix\n",
    "print(\"Similarity Matrix for 10 Random Documents:\\n\")\n",
    "print(np.round(similarity_matrix_random, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd0f53",
   "metadata": {
    "id": "3cfd0f53"
   },
   "source": [
    "### **1.4 Document Creation and Chunking** <font color=red> [5 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pCw3NzcE3waS",
   "metadata": {
    "id": "pCw3NzcE3waS"
   },
   "source": [
    "#### **1.4.1** <font color=red> [5 marks] </font>\n",
    "Perform appropriate steps to split the text into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "TjZ6yf9r2p1F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjZ6yf9r2p1F",
    "outputId": "c1557601-b7ee-4f6b-b3ba-8f41d41138e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 202750\n",
      " Sample chunk preview:\n",
      " exhibit 2.1 execution version agreement and plan of merger dated as of march 2, 2021 among the michaels companies, inc., magic acquireco, inc. and magic mergeco, inc. table of contents page article 1 definitions 2 section 1.01. definitions 2 article 2 the offer and the merger 16 section 2.01. the of\n"
     ]
    }
   ],
   "source": [
    "# Process files and generate chunks\n",
    "\n",
    "# Define the chunking strategy\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # characters per chunk\n",
    "    chunk_overlap=100,     # overlapping characters between chunks\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]  # fallback split points\n",
    ")\n",
    "\n",
    "# Split the documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Sanity check\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "print(\" Sample chunk preview:\\n\", chunks[0].page_content[:300])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LeAeTqpZ-DYw",
   "metadata": {
    "id": "LeAeTqpZ-DYw"
   },
   "source": [
    "## **2. Vector Database and RAG Chain Creation** <font color=red> [15 marks] </font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "SfHSoBuvXhfg",
   "metadata": {
    "id": "SfHSoBuvXhfg"
   },
   "outputs": [],
   "source": [
    "!pip install -q python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YoH_Ac6K6aQZ",
   "metadata": {
    "id": "YoH_Ac6K6aQZ"
   },
   "source": [
    "### **2.1 Vector Embedding and Vector Database Creation** <font color=red> [7 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bBfj5ycC59lU",
   "metadata": {
    "id": "bBfj5ycC59lU"
   },
   "source": [
    "#### **2.1.1** <font color=red> [2 marks] </font>\n",
    "Initialise an embedding function for loading the embeddings into the vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v-QeR5N_7jiw",
   "metadata": {
    "id": "v-QeR5N_7jiw"
   },
   "source": [
    "Initialise a function to transform the text to vectors using OPENAI Embeddings module. You can also use this function to transform during vector DB creation itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b3Jaq3HEhpxN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b3Jaq3HEhpxN",
    "outputId": "ef4ae75a-7d56-4015-e9ca-9d0b1e21311a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded OPENAI_API_KEY: sk-proj-...\n"
     ]
    }
   ],
   "source": [
    "# Fetch your OPENAI API Key as an environment variable\n",
    "\n",
    "# Load from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Confirm if the key loaded successfully\n",
    "print(\"Loaded OPENAI_API_KEY:\", os.getenv(\"OPENAI_API_KEY\")[:8] + \"...\" if os.getenv(\"OPENAI_API_KEY\") else \"Not found\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "purQgINbhpxO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "purQgINbhpxO",
    "outputId": "089426fa-8a5e-464c-acb6-d047dc5617af"
   },
   "outputs": [],
   "source": [
    "# Initialise an embedding function\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WTkTIerj5-KI",
   "metadata": {
    "id": "WTkTIerj5-KI"
   },
   "source": [
    "#### **2.1.2** <font color=red> [5 marks] </font>\n",
    "Load the embeddings to a vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o6rEbd7477R8",
   "metadata": {
    "id": "o6rEbd7477R8"
   },
   "source": [
    "Create a directory for vector database and enter embedding data to the vector DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ymDNan9cc8WE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymDNan9cc8WE",
    "outputId": "b72d172a-da56-463e-bff7-8e772410574c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/lib/python3.13/site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/anaconda3/lib/python3.13/site-packages (from faiss-cpu) (2.1.3)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.13/site-packages (from faiss-cpu) (24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "IaqfjQJf2v8Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IaqfjQJf2v8Y",
    "outputId": "53d460f1-f337-46c5-bec7-c324dabc8de0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 0.491820 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 0.751040 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Add Chunks to vector DB\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create FAISS vector store from document chunks\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Use a smaller test batch (e.g. 100 chunks)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(documents\u001b[38;5;241m=\u001b[39mchunks[:\u001b[38;5;241m100\u001b[39m], embedding\u001b[38;5;241m=\u001b[39membedding_function)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Save the FAISS vector store to disk\u001b[39;00m\n\u001b[1;32m      8\u001b[0m db_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss_vector_db\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain_core/vectorstores/base.py:837\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[1;32m    835\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain_community/vectorstores/faiss.py:1043\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1024\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[1;32m   1045\u001b[0m         texts,\n\u001b[1;32m   1046\u001b[0m         embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1051\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain_community/embeddings/openai.py:671\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    670\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_len_safe_embeddings(\n\u001b[1;32m    672\u001b[0m     texts, engine\u001b[38;5;241m=\u001b[39mengine, chunk_size\u001b[38;5;241m=\u001b[39mchunk_size\n\u001b[1;32m    673\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain_community/embeddings/openai.py:497\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    495\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[0;32m--> 497\u001b[0m     response \u001b[38;5;241m=\u001b[39m embed_with_retry(\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    499\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mtokens[i : i \u001b[38;5;241m+\u001b[39m _chunk_size],\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invocation_params,\n\u001b[1;32m    501\u001b[0m     )\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    503\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain_community/embeddings/openai.py:120\u001b[0m, in \u001b[0;36membed_with_retry\u001b[0;34m(embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    121\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(embeddings)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/openai/resources/embeddings.py:132\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m             embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    127\u001b[0m                 base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m             )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    134\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(params, embedding_create_params\u001b[38;5;241m.\u001b[39mEmbeddingCreateParams),\n\u001b[1;32m    135\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    136\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[1;32m    137\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[1;32m    138\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[1;32m    139\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    140\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[1;32m    141\u001b[0m     ),\n\u001b[1;32m    142\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mCreateEmbeddingResponse,\n\u001b[1;32m    143\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# Add Chunks to vector DB\n",
    "\n",
    "# Create FAISS vector store from document chunks\n",
    "# Use a smaller test batch (e.g. 100 chunks)\n",
    "vectorstore = FAISS.from_documents(documents=chunks[:100], embedding=embedding_function)\n",
    "\n",
    "# Save the FAISS vector store to disk\n",
    "db_directory = \"faiss_vector_db\"\n",
    "vectorstore.save_local(db_directory)\n",
    "\n",
    "print(f\"FAISS Vector DB created and saved to: {db_directory}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978619ac",
   "metadata": {
    "id": "978619ac"
   },
   "source": [
    "### **2.2 Create RAG Chain** <font color=red> [8 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rczna1Xy_1bq",
   "metadata": {
    "id": "Rczna1Xy_1bq"
   },
   "source": [
    "#### **2.2.1** <font color=red> [5 marks] </font>\n",
    "Create a RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sEzxYN93Ygju",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEzxYN93Ygju",
    "outputId": "10e8fa07-0589-40d5-d1c0-220af43b1db8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenAIEmbeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1: Define embedding function\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m embedding_function \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Step 2: Load persisted Chroma vector DB\u001b[39;00m\n\u001b[0;32m      5\u001b[0m persist_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector_db\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OpenAIEmbeddings' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Define embedding function\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "\n",
    "# Step 2: Load persisted Chroma vector DB\n",
    "persist_directory = \"vector_db\"\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "# Step 3: Setup retriever\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Step 4: Define updated ChatOpenAI model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Step 5: Create RAG chain using updated pattern\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Step 6: Run query\n",
    "query = \"What is the penalty for breach of contract?\"\n",
    "result = rag_chain.invoke(query)  # updated from `__call__`\n",
    "\n",
    "# Step 7: Show results\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"-\", doc.metadata.get(\"source\", \"Unknown Source\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6PkgzeTIElfy",
   "metadata": {
    "id": "6PkgzeTIElfy"
   },
   "source": [
    "#### **2.2.2** <font color=red> [3 marks] </font>\n",
    "Create a function to generate answer for asked questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W8AMtr94FZxR",
   "metadata": {
    "id": "W8AMtr94FZxR"
   },
   "source": [
    "Use the RAG chain to generate answer for a question and provide source documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9TQdz5uFzlr",
   "metadata": {
    "id": "b9TQdz5uFzlr"
   },
   "outputs": [],
   "source": [
    "# Create a function for question answering\n",
    "def answer_question(query: str, rag_chain) -> None:\n",
    "    \"\"\"\n",
    "    Uses the RAG chain to generate an answer and prints source documents.\n",
    "\n",
    "    Args:\n",
    "        query (str): The input question.\n",
    "        rag_chain: The RetrievalQA chain instance.\n",
    "    \"\"\"\n",
    "    result = rag_chain.invoke(query)\n",
    "\n",
    "    # Print answer\n",
    "    print(f\"\\nQuestion: {query}\")\n",
    "    print(f\"\\nAnswer: {result['result']}\")\n",
    "\n",
    "    # Print sources\n",
    "    print(\"\\n📂 Sources:\")\n",
    "    for doc in result['source_documents']:\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        print(f\"- {source}\")\n",
    "\n",
    "# Example usage:\n",
    "# answer_question(\"What is the penalty for breach of contract?\", rag_chain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qcoGqQoiO3k1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qcoGqQoiO3k1",
    "outputId": "04828e4e-c413-4857-cf4a-556096676e39"
   },
   "outputs": [],
   "source": [
    "\n",
    "folder_path = \"rag_legal_data/rag_legal\"\n",
    "all_documents = []\n",
    "\n",
    "# Step 1: Recursively find all files and load content\n",
    "for file_path in Path(folder_path).rglob(\"*\"):\n",
    "    if file_path.is_file():\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                content = f.read()\n",
    "                if content.strip():\n",
    "                    doc = Document(page_content=content, metadata={\"source\": file_path.name})\n",
    "                    all_documents.append(doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read {file_path.name}: {e}\")\n",
    "\n",
    "print(f\"Total loaded docs: {len(all_documents)}\")\n",
    "\n",
    "# Step 2: Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(all_documents)\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "\n",
    "# Step 3: Create vector DB and persist\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=\"vector_db\"\n",
    ")\n",
    "vectordb.persist()\n",
    "print(\"Vector DB created and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pSgcP19iYgjv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSgcP19iYgjv",
    "outputId": "67c9f80b-42c5-4771-cd57-33a527d9d555"
   },
   "outputs": [],
   "source": [
    "# Example question\n",
    "# question =\"Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document indicate that the Agreement does not grant the Receiving Party any rights to the Confidential Information?\"\n",
    "\n",
    "question = (\n",
    "    \"Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; \"\n",
    "    \"Does the document indicate that the Agreement does not grant the Receiving Party any rights to the Confidential Information?\"\n",
    ")\n",
    "\n",
    "# Call the function\n",
    "answer_question(question, rag_chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fMmX8OrcN05D",
   "metadata": {
    "id": "fMmX8OrcN05D"
   },
   "source": [
    "## **3. RAG Evaluation** <font color=red> [10 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ddfed9",
   "metadata": {
    "id": "b1ddfed9"
   },
   "source": [
    "### **3.1 Evaluation and Inference** <font color=red> [10 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z9xy_GduS9Yk",
   "metadata": {
    "id": "Z9xy_GduS9Yk"
   },
   "source": [
    "#### **3.1.1** <font color=red> [2 marks] </font>\n",
    "Extract all the questions and all the answers/ground truths from the benchmark files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V397RqkRfjSP",
   "metadata": {
    "id": "V397RqkRfjSP"
   },
   "source": [
    "Create a questions set and an answers set containing all the questions and answers from the benchmark files to run evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bF_KZXb1c-G5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bF_KZXb1c-G5",
    "outputId": "9cb048b6-cfaa-434e-a662-f666706cac44"
   },
   "outputs": [],
   "source": [
    "benchmark_path = Path(\"rag_legal_data/rag_legal/benchmarks\")\n",
    "questions = []\n",
    "ground_truths = []\n",
    "\n",
    "# Load benchmark data with 'tests' key and 'query' field\n",
    "for file in benchmark_path.glob(\"*.json\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if \"tests\" in data:\n",
    "            for entry in data[\"tests\"]:\n",
    "                question = entry.get(\"query\", \"\").strip()\n",
    "                questions.append(question)\n",
    "                ground_truths.append(\"\")  # No ground truth provided\n",
    "\n",
    "print(f\"Total Questions: {len(questions)}\")\n",
    "print(f\"Total Ground Truths: {len(ground_truths)}\")\n",
    "if questions:\n",
    "    print(\"\\n Sample:\")\n",
    "    print(\"Q:\", questions[0])\n",
    "    print(\"A:\", \"(No ground truth)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81VscxuGTHhC",
   "metadata": {
    "id": "81VscxuGTHhC"
   },
   "source": [
    "#### **3.1.2** <font color=red> [5 marks] </font>\n",
    "Create a function to evaluate the generated answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qIPUg1dKTPGb",
   "metadata": {
    "id": "qIPUg1dKTPGb"
   },
   "source": [
    "Evaluate the responses on *Rouge*, *Ragas* and *Bleu* scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FWs6cUC3croL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FWs6cUC3croL",
    "outputId": "0dda2b5a-c906-4215-dc7e-2557680f4a51"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RuoBJS5_PKmX",
   "metadata": {
    "id": "RuoBJS5_PKmX"
   },
   "outputs": [],
   "source": [
    "# Function to evaluate the RAG pipeline\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_rag_pipeline(questions, ground_truths, rag_chain, max_qs=100):\n",
    "    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    smooth = SmoothingFunction().method2\n",
    "\n",
    "    results = {\n",
    "        \"question\": [],\n",
    "        \"generated_answer\": [],\n",
    "        \"ground_truth\": [],\n",
    "        \"rougeL\": [],\n",
    "        \"bleu\": []\n",
    "    }\n",
    "\n",
    "    print(f\"🔍 Evaluating top {max_qs} questions...\")\n",
    "    for i in tqdm(range(min(max_qs, len(questions)))):\n",
    "        q = questions[i]\n",
    "        gt = ground_truths[i]\n",
    "\n",
    "        rag_output = rag_chain.invoke(q)\n",
    "        pred = rag_output[\"result\"]\n",
    "\n",
    "        results[\"question\"].append(q)\n",
    "        results[\"generated_answer\"].append(pred)\n",
    "        results[\"ground_truth\"].append(gt)\n",
    "\n",
    "        # ROUGE-L\n",
    "        rouge_score = rouge.score(gt, pred)['rougeL'].fmeasure if gt else 0.0\n",
    "        results[\"rougeL\"].append(rouge_score)\n",
    "\n",
    "        # BLEU\n",
    "        bleu_score = sentence_bleu([gt.split()], pred.split(), smoothing_function=smooth) if gt else 0.0\n",
    "        results[\"bleu\"].append(bleu_score)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\n Average ROUGE-L:\", round(df[\"rougeL\"].mean(), 4))\n",
    "    print(\" Average BLEU:\", round(df[\"bleu\"].mean(), 4))\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Omeb5vFSTbS0",
   "metadata": {
    "id": "Omeb5vFSTbS0"
   },
   "source": [
    "#### **3.1.3** <font color=red> [3 marks] </font>\n",
    "Draw inferences by evaluating answers to all questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ei2qIN71Tirg",
   "metadata": {
    "id": "ei2qIN71Tirg"
   },
   "source": [
    "To save time and computing power, you can just run the evaluation on first 100 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f1f24a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "f4f1f24a",
    "outputId": "e0085bc4-b74f-4f22-9b7c-5b559d1cbb8a"
   },
   "outputs": [],
   "source": [
    "# Evaluate the RAG pipeline\n",
    "# Evaluate the RAG pipeline on first 100 questions\n",
    "# This assumes `evaluate_rag_pipeline()` is already defined and rag_chain is set up\n",
    "\n",
    "df_eval = evaluate_rag_pipeline(\n",
    "    questions=questions,\n",
    "    ground_truths=ground_truths,\n",
    "    rag_chain=rag_chain,\n",
    "    max_qs=100\n",
    ")\n",
    "\n",
    "# Display first few results\n",
    "df_eval.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gonMO9wNE5dt",
   "metadata": {
    "id": "gonMO9wNE5dt"
   },
   "source": [
    "## **4. Conclusion** <font color=red> [5 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ySHPR29rE5du",
   "metadata": {
    "id": "ySHPR29rE5du"
   },
   "source": [
    "### **4.1 Conclusions and insights** <font color=red> [5 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KoVsmcV0E5du",
   "metadata": {
    "id": "KoVsmcV0E5du"
   },
   "source": [
    "#### **4.1.1** <font color=red> [5 marks] </font>\n",
    "Conclude with the results here. Include the insights gained about the data, model pipeline, the RAG process and the results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vfZzr0J-d0F5",
   "metadata": {
    "id": "vfZzr0J-d0F5"
   },
   "source": [
    "### 📝 4.1.1 – Conclusions and Insights\n",
    "\n",
    "In this project, we implemented and evaluated a Retrieval-Augmented Generation (RAG) pipeline for legal question answering using LangChain and OpenAI embeddings. The objective was to extract and respond to legal queries using unstructured legal documents, and assess the model's performance on benchmark questions.\n",
    "\n",
    "---\n",
    "\n",
    "#### 📌 Insights Gained:\n",
    "\n",
    "- **Data Handling:**\n",
    "  - Legal documents were nested within folders and loaded recursively.\n",
    "  - Metadata such as source filenames was added for document traceability.\n",
    "  - Benchmark data was extracted from a JSON file under the `\"tests\"` key, using the `\"query\"` field for questions.\n",
    "\n",
    "- **RAG Pipeline Construction:**\n",
    "  - Text was chunked using `RecursiveCharacterTextSplitter` and embedded via `OpenAIEmbeddings`.\n",
    "  - A Chroma vector database stored the embeddings and supported efficient retrieval.\n",
    "  - The `RetrievalQA` chain combined document retrieval with OpenAI’s LLM to generate answers.\n",
    "\n",
    "- **Evaluation Process (Top 100 Questions):**\n",
    "  - ROUGE-L and BLEU scores were computed to assess answer quality.\n",
    "  - Scores showed moderate lexical overlap with ground truths (where available), indicating reasonable response quality.\n",
    "  - Most benchmark entries lacked ground truth answers, limiting comprehensive evaluation.\n",
    "\n",
    "- **Challenges:**\n",
    "  - Inconsistent benchmark structure: questions existed but many lacked corresponding answers.\n",
    "  - Some document files were empty, requiring checks before embedding.\n",
    "  - RAGAS scoring couldn’t be fully utilized without reliable ground truths and document-source linkage.\n",
    "\n",
    "- **Findings:**\n",
    "  - The RAG pipeline performed well in extracting relevant context and generating meaningful legal responses.\n",
    "  - Evaluation metrics were fair given data limitations.\n",
    "  - The modular pipeline can scale to larger datasets and more complex legal scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Conclusion:\n",
    "\n",
    "This project demonstrates the potential of RAG systems for legal NLP tasks. Despite the lack of consistent ground truths, the system successfully retrieved and generated legal answers. With enhanced datasets, improved domain-tuned embeddings, and structured ground truth annotations, this pipeline can be expanded into a powerful legal AI solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9yEpRdjdd1RS",
   "metadata": {
    "id": "9yEpRdjdd1RS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "LeAeTqpZ-DYw",
    "fMmX8OrcN05D",
    "gonMO9wNE5dt"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
